{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1npOjkcp-A-KiCK3MN50ligoyMxKFXgw2","authorship_tag":"ABX9TyMguA/ULG0+gZuiLp/tcId1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlkjlmhJFmaQ","executionInfo":{"status":"ok","timestamp":1691556275983,"user_tz":-120,"elapsed":49662,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"2e4ddb5c-20af-410c-f476-136437102fa1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285397 sha256=6d0a9f93871559188cdb489a8bfbb5ff54395de46efeab2b5d61c71c5eafe89c\n","  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.1\n"]}],"source":["!pip install pyspark"]},{"cell_type":"code","source":["import pyspark\n","import pandas as pd"],"metadata":{"id":"EcStLr4DGRAa","executionInfo":{"status":"ok","timestamp":1691556276710,"user_tz":-120,"elapsed":762,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["First we have to create a spark session and also provide a name for the session. In this case, the session name is practice."],"metadata":{"id":"NgJy-FgwoeTW"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('Practice').getOrCreate()"],"metadata":{"id":"DqEaEdLDGQ52","executionInfo":{"status":"ok","timestamp":1691556287343,"user_tz":-120,"elapsed":10646,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["spark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"SGPtV3n1GQ0j","executionInfo":{"status":"ok","timestamp":1691556288875,"user_tz":-120,"elapsed":1546,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"10f80f4e-f589-4321-ee26-a9257dddd480"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7e97f06d8160>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://f4d71a0a6b1a:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.4.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Practice</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# To read a csv using spark\n","# Show is used to print the entire csv\n","spark.read.csv('/content/drive/MyDrive/Datasets/Pyspark/test1.csv').show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zyYdm6e-GQxg","executionInfo":{"status":"ok","timestamp":1691556301736,"user_tz":-120,"elapsed":12876,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"21dc37eb-83f3-40dc-d569-93f2f9f7bdd6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+---+----------+------+\n","|      _c0|_c1|       _c2|   _c3|\n","+---------+---+----------+------+\n","|     Name|age|Experience|Salary|\n","|    Krish| 31|        10| 30000|\n","|Sudhanshu| 30|         8| 25000|\n","|    Sunny| 29|         4| 20000|\n","|     Paul| 24|         3| 20000|\n","|   Harsha| 21|         1| 15000|\n","|  Shubham| 23|         2| 18000|\n","+---------+---+----------+------+\n","\n"]}]},{"cell_type":"code","source":["# option('header', 'true') specifies to use first row as header\n","df_spark = spark.read.option('header', 'true').csv('/content/drive/MyDrive/Datasets/Pyspark/test1.csv')"],"metadata":{"id":"TnYLmeFuGQss","executionInfo":{"status":"ok","timestamp":1691556302462,"user_tz":-120,"elapsed":486,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["df_spark.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-MnGdVP-GQrK","executionInfo":{"status":"ok","timestamp":1691556302866,"user_tz":-120,"elapsed":408,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"ca569ab9-d224-43f5-d2d2-6d205858a7a0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+---+----------+------+\n","|     Name|age|Experience|Salary|\n","+---------+---+----------+------+\n","|    Krish| 31|        10| 30000|\n","|Sudhanshu| 30|         8| 25000|\n","|    Sunny| 29|         4| 20000|\n","|     Paul| 24|         3| 20000|\n","|   Harsha| 21|         1| 15000|\n","|  Shubham| 23|         2| 18000|\n","+---------+---+----------+------+\n","\n"]}]},{"cell_type":"code","source":["type(df_spark)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8_jORUsGQn2","executionInfo":{"status":"ok","timestamp":1691556302867,"user_tz":-120,"elapsed":13,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"c77d408e-007d-448b-a2d3-e8efbba5d98f"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pyspark.sql.dataframe.DataFrame"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# To check the schema. Similar to df.info() in pandas\n","# It gives information about the datatypes of columns in the dataframe\n","# By default all the columns are considered as string\n","df_spark.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kfi7TQTZJTPV","executionInfo":{"status":"ok","timestamp":1691556302867,"user_tz":-120,"elapsed":9,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"124083fa-7c72-4269-d347-73e3a6b66e3e"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- Name: string (nullable = true)\n"," |-- age: string (nullable = true)\n"," |-- Experience: string (nullable = true)\n"," |-- Salary: string (nullable = true)\n","\n"]}]},{"cell_type":"code","source":["# InferSchema option specifies to consider all the columns in its original datatype\n","# nullable = true tells that column can contain null values\n","spark.read.option('header', 'true').csv('/content/drive/MyDrive/Datasets/Pyspark/test1.csv', inferSchema=True).printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GaUxoLSRJTLx","executionInfo":{"status":"ok","timestamp":1691556303840,"user_tz":-120,"elapsed":978,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"24c146cd-b4e6-43d4-e19c-4b417c5a05bc"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- Name: string (nullable = true)\n"," |-- age: integer (nullable = true)\n"," |-- Experience: integer (nullable = true)\n"," |-- Salary: integer (nullable = true)\n","\n"]}]},{"cell_type":"markdown","source":["### Writing above steps in a clean format"],"metadata":{"id":"ouljI3GmrNkc"}},{"cell_type":"code","source":["df_pyspark = spark.read.csv('/content/drive/MyDrive/Datasets/Pyspark/test1.csv', header=True, inferSchema=True)\n","df_pyspark.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2p_zFmGUJTKM","executionInfo":{"status":"ok","timestamp":1691556305017,"user_tz":-120,"elapsed":1178,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"047dc287-9c9c-4fe5-95de-f781f06e3b3d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+---+----------+------+\n","|     Name|age|Experience|Salary|\n","+---------+---+----------+------+\n","|    Krish| 31|        10| 30000|\n","|Sudhanshu| 30|         8| 25000|\n","|    Sunny| 29|         4| 20000|\n","|     Paul| 24|         3| 20000|\n","|   Harsha| 21|         1| 15000|\n","|  Shubham| 23|         2| 18000|\n","+---------+---+----------+------+\n","\n"]}]},{"cell_type":"code","source":["# To get the list of available columns in dataframe\n","df_pyspark.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YHKsgl7iJTIH","executionInfo":{"status":"ok","timestamp":1691556305381,"user_tz":-120,"elapsed":366,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"c66301d4-4a47-402d-a2f2-c2c27a99accf"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Name', 'age', 'Experience', 'Salary']"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# Selecting only particular columns in df\n","df_pyspark.select(['Name', 'Salary']).head(3)\n","df_pyspark.select(['Name', 'Salary']).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3I99zO9TJTGU","executionInfo":{"status":"ok","timestamp":1691556306821,"user_tz":-120,"elapsed":1442,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"0e7a3c75-1943-40bf-83eb-d171dbb2b253"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+------+\n","|     Name|Salary|\n","+---------+------+\n","|    Krish| 30000|\n","|Sudhanshu| 25000|\n","|    Sunny| 20000|\n","|     Paul| 20000|\n","|   Harsha| 15000|\n","|  Shubham| 18000|\n","+---------+------+\n","\n"]}]},{"cell_type":"code","source":["# To just get the datatypes of columns\n","df_pyspark.dtypes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A3UrwaBOJTCn","executionInfo":{"status":"ok","timestamp":1691556306821,"user_tz":-120,"elapsed":4,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"7828f6b8-4458-4177-c115-868d8791c640"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Name', 'string'), ('age', 'int'), ('Experience', 'int'), ('Salary', 'int')]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# To describe the df just like in pandas\n","df_pyspark.describe().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b194KLkAJTBI","executionInfo":{"status":"ok","timestamp":1691556311335,"user_tz":-120,"elapsed":4516,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"bcf65f9b-7dfa-42ec-922a-8c85fb750ac2"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+------+------------------+-----------------+------------------+\n","|summary|  Name|               age|       Experience|            Salary|\n","+-------+------+------------------+-----------------+------------------+\n","|  count|     6|                 6|                6|                 6|\n","|   mean|  null|26.333333333333332|4.666666666666667|21333.333333333332|\n","| stddev|  null| 4.179314138308661|3.559026084010437| 5354.126134736337|\n","|    min|Harsha|                21|                1|             15000|\n","|    max| Sunny|                31|               10|             30000|\n","+-------+------+------------------+-----------------+------------------+\n","\n"]}]},{"cell_type":"markdown","source":["## Adding and Dropping column"],"metadata":{"id":"ZzCK5mJFt-G_"}},{"cell_type":"code","source":["# Adding a column to df\n","df_pyspark = df_pyspark.withColumn('Experience after 2yrs', df_pyspark['Experience']+2)\n","df_pyspark.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5jEl5XZ-JS9q","executionInfo":{"status":"ok","timestamp":1691556311710,"user_tz":-120,"elapsed":387,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"9cf2ca07-97c0-41e6-fdf1-c52276d68a12"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+---+----------+------+---------------------+\n","|     Name|age|Experience|Salary|Experience after 2yrs|\n","+---------+---+----------+------+---------------------+\n","|    Krish| 31|        10| 30000|                   12|\n","|Sudhanshu| 30|         8| 25000|                   10|\n","|    Sunny| 29|         4| 20000|                    6|\n","|     Paul| 24|         3| 20000|                    5|\n","|   Harsha| 21|         1| 15000|                    3|\n","|  Shubham| 23|         2| 18000|                    4|\n","+---------+---+----------+------+---------------------+\n","\n"]}]},{"cell_type":"code","source":["# drop a column from df\n","df_pyspark = df_pyspark.drop('Experience after 2yrs')\n","df_pyspark.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O9TGwlAeJS72","executionInfo":{"status":"ok","timestamp":1691556311943,"user_tz":-120,"elapsed":235,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"a9fb7e48-ae5f-4d71-f750-a8f141bb719d"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+---+----------+------+\n","|     Name|age|Experience|Salary|\n","+---------+---+----------+------+\n","|    Krish| 31|        10| 30000|\n","|Sudhanshu| 30|         8| 25000|\n","|    Sunny| 29|         4| 20000|\n","|     Paul| 24|         3| 20000|\n","|   Harsha| 21|         1| 15000|\n","|  Shubham| 23|         2| 18000|\n","+---------+---+----------+------+\n","\n"]}]},{"cell_type":"code","source":["# Renaming a column\n","df_pyspark = df_pyspark.withColumnRenamed('Name', 'First name')\n","df_pyspark.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zlpElKAgJS3B","executionInfo":{"status":"ok","timestamp":1691556312316,"user_tz":-120,"elapsed":374,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"037242e8-2450-4b74-c8c8-6fe7b018c443"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+---+----------+------+\n","|First name|age|Experience|Salary|\n","+----------+---+----------+------+\n","|     Krish| 31|        10| 30000|\n","| Sudhanshu| 30|         8| 25000|\n","|     Sunny| 29|         4| 20000|\n","|      Paul| 24|         3| 20000|\n","|    Harsha| 21|         1| 15000|\n","|   Shubham| 23|         2| 18000|\n","+----------+---+----------+------+\n","\n"]}]},{"cell_type":"markdown","source":["## Dropping null values"],"metadata":{"id":"Ve1WNKAJrbhZ"}},{"cell_type":"code","source":["# drop function contains 3 parameters.\n","# how = 'all or any' specifies to drop a row if only all values of row are empty or any value of row is empty.\n","# df_pyspark.na.drop(how='all').show()\n","df_pyspark.na.drop(how='any').show()\n"],"metadata":{"id":"oBX2TzNDJSzF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691557167202,"user_tz":-120,"elapsed":632,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"8dd1d2c8-6c20-46bf-e867-a542f440ec02"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+---+----------+------+\n","|First name|age|Experience|Salary|\n","+----------+---+----------+------+\n","|     Krish| 31|        10| 30000|\n","| Sudhanshu| 30|         8| 25000|\n","|     Sunny| 29|         4| 20000|\n","|      Paul| 24|         3| 20000|\n","|    Harsha| 21|         1| 15000|\n","|   Shubham| 23|         2| 18000|\n","+----------+---+----------+------+\n","\n"]}]},{"cell_type":"code","source":["# drop null values with threshold.\n","# thresh = 2 checks if a row contains atleast 2 non null values. If not, it is dropped.\n","df_pyspark.na.drop(how='any', thresh=2).show()\n"],"metadata":{"id":"SRbTn3CTJSxg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691557294361,"user_tz":-120,"elapsed":630,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"6fe300b1-2d52-4b5b-a918-4509ee5ebc83"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+---+----------+------+\n","|First name|age|Experience|Salary|\n","+----------+---+----------+------+\n","|     Krish| 31|        10| 30000|\n","| Sudhanshu| 30|         8| 25000|\n","|     Sunny| 29|         4| 20000|\n","|      Paul| 24|         3| 20000|\n","|    Harsha| 21|         1| 15000|\n","|   Shubham| 23|         2| 18000|\n","+----------+---+----------+------+\n","\n"]}]},{"cell_type":"code","source":["# Droppping null values based on subset\n","df_pyspark.na.drop(how='any', subset=['age', 'Experience']).show()"],"metadata":{"id":"jeXleIFqJSuL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691557377966,"user_tz":-120,"elapsed":608,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"982caae3-8565-47ca-b93c-fdbed43040a7"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+---+----------+------+\n","|First name|age|Experience|Salary|\n","+----------+---+----------+------+\n","|     Krish| 31|        10| 30000|\n","| Sudhanshu| 30|         8| 25000|\n","|     Sunny| 29|         4| 20000|\n","|      Paul| 24|         3| 20000|\n","|    Harsha| 21|         1| 15000|\n","|   Shubham| 23|         2| 18000|\n","+----------+---+----------+------+\n","\n"]}]},{"cell_type":"markdown","source":["## Fill missing values"],"metadata":{"id":"oJWk5Se0riyJ"}},{"cell_type":"code","source":["# This code fills the nan values in Experience column with word 'Missing value'\n","df_pyspark.na.fill('Missing value', 'Experience').show()"],"metadata":{"id":"_yESO7LGJSsg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691557531448,"user_tz":-120,"elapsed":603,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"3bd0459b-945d-4974-c671-76c001504276"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+---+----------+------+\n","|First name|age|Experience|Salary|\n","+----------+---+----------+------+\n","|     Krish| 31|        10| 30000|\n","| Sudhanshu| 30|         8| 25000|\n","|     Sunny| 29|         4| 20000|\n","|      Paul| 24|         3| 20000|\n","|    Harsha| 21|         1| 15000|\n","|   Shubham| 23|         2| 18000|\n","+----------+---+----------+------+\n","\n"]}]},{"cell_type":"code","source":["# Using imputer function to fill null values\n","from pyspark.ml.feature import Imputer\n","\n","imputer = Imputer(\n","    inputCols=['age', 'Experience', 'Salary'],\n","    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n","    ).setStrategy(\"mean\")"],"metadata":{"id":"pWEUHIOKrBBX","executionInfo":{"status":"ok","timestamp":1691557775812,"user_tz":-120,"elapsed":249,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["imputer.fit(df_pyspark).transform(df_pyspark).show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5USbbQSSrA_J","executionInfo":{"status":"ok","timestamp":1691557778933,"user_tz":-120,"elapsed":1708,"user":{"displayName":"Naveen Kumar","userId":"04971356055513306582"}},"outputId":"2f8f1190-b53c-4666-d8ca-9e639d7e4093"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+---+----------+------+-----------+------------------+--------------+\n","|First name|age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n","+----------+---+----------+------+-----------+------------------+--------------+\n","|     Krish| 31|        10| 30000|         31|                10|         30000|\n","| Sudhanshu| 30|         8| 25000|         30|                 8|         25000|\n","|     Sunny| 29|         4| 20000|         29|                 4|         20000|\n","|      Paul| 24|         3| 20000|         24|                 3|         20000|\n","|    Harsha| 21|         1| 15000|         21|                 1|         15000|\n","|   Shubham| 23|         2| 18000|         23|                 2|         18000|\n","+----------+---+----------+------+-----------+------------------+--------------+\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"GRgbXESLrA9A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VVoBSf7urA7L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UuHwjehurA28"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Y2U6WO90rA1G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GGL3PFd7rAx8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gidrPqArrAwL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"l7xTBoLhrAsR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AVxAGvggrAqI"},"execution_count":null,"outputs":[]}]}